---
title: "Predictive Analytics in Long-Term Care"
output: 
  html_notebook: 
    toc: yes
---



# Prepare Notebook

Run this "Prepare Notebook"" section before any of the algorithms

### Load Libraries

```{r}
## Load Libraries and prepare notebook

# Load libraries
library(dplyr)
library(readr)
library(tidyr)
library(ggplot2)
library(keras)
use_condaenv("r-tensorflow") # depending on your setup, this line might not be necessary
library(xgboost)
library(speedglm)
library(purrr)
library(glmnet)

# doMC is not supported in Windows.  For Windows delete the next two lines.
require(doMC) # Not available in Windows
registerDoMC(cores=10) # Set to the available no. of cores


## Prepare Workbook
p0=paste0
working_dir = p0(getwd(),"/")
data_dir = p0(working_dir, "data/")
output_dir = p0(working_dir, "output/")

## Load utilities
source(p0(working_dir, 'utilities.R'))

```

### Important Formulae

```{r}

#'  Poisson Negative Log Likelihood
poisson_neg_log_lik = function(y_pred, y_true, eps=0){
  mean(y_pred - y_true * log(y_pred+eps))
}

```


# Sample Formula 

```{r}
# Get Data 
incidence = get_incidence()
  
###########################
### Sample Formula Data
###########################

formula = Count_NH ~  (IssueYear + IncurredAgeBucket + I(IncurredAgeBucket^2))
design = model.matrix(formula, incidence$train)
head(design)

```


# Generalized Linear Model (GLM)


### Scale Data

```{r}
incidence = get_incidence() # full data
incidence = get_incidence(train_val_test = c(.1,.1,.1)) #Select only 10% of the data


# Scale all values except exposure and counts and response variables
incidence_scaled = scale_data(incidence)
incidence = incidence_scaled$incidence; 
scale_mean = incidence_scaled$scale_mean; 
scale_sd = incidence_scaled$scale_sd
rm(incidence_scaled)
```


### Define GLM Formulae and Method

This section defines three formulae options.  The GLM can be run using the standad R version as well as speedglm which is a much faster version and produces similar (although not quite as accurate) results.

```{r}


# Define various formulae

# Main GLM Formula
# excludes StateAbbr, RateIncreaseFlag, IssueAgeBucket
formula = Count_NH ~ offset(log(ActiveExposure)) +
  (Gender + IssueYear + IncurredAgeBucket  + PolicyYear + Marital_Status + Prem_Class + 
      Underwriting_Type + Cov_Type_Bucket + TQ_Status + NH_Orig_Daily_Ben_Bucket + NH_Ben_Period_Bucket + 
      NH_EP_Bucket  + Region)^2   + Infl_Rider + I(Duration^2) 

# Full formula but without Any Interactions
formula_no_interactions = Count_NH ~ offset(log(ActiveExposure)) +
  (Gender + IssueYear + IncurredAgeBucket  + PolicyYear + Marital_Status + Prem_Class + 
      Underwriting_Type + Cov_Type_Bucket + TQ_Status + NH_Orig_Daily_Ben_Bucket + NH_Ben_Period_Bucket + 
      NH_EP_Bucket  + Region)   + Infl_Rider + I(Duration^2) 

# SOA Formula
formula_soa = Count_NH ~ offset(log(ActiveExposure)) +
  NH_Ben_Period_Bucket + NH_Orig_Daily_Ben_Bucket:Region + 
  PolicyYear:Prem_Class+ PolicyYear:Underwriting_Type + PolicyYear:TQ_Status +
  IncurredAgeBucket:Cov_Type_Bucket + IncurredAgeBucket:NH_EP_Bucket + IncurredAgeBucket:Gender +
  IncurredAgeBucket:Marital_Status


#' Wrapper around glm function
#' 
#' @param formula Formula to use
#' @param speed Default is FALSE, for standard GLM formula, TREU for speedglm formula

run_glm = function(formula, speed=FALSE){
  # Run GLM Model
  a=Sys.time()
  if (!speed){
    mod_glm = glm(formula = formula, data = incidence$train, family = poisson)
  } else {
    mod_glm = speedglm(formula = formula, data = incidence$train, family = poisson(),tol.values=1e-7, maxit= 50)
  }
  b=Sys.time(); time_taken = (b-a)
  
  
  # Perform predictions on validation set
  preds = predict(object = mod_glm, newdata = incidence$val, type = 'response')
  incidence$val$preds = preds


  # Produce Plots
  res = incidence$val %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, 
         expected = preds / AdjustedExposure)

  rate_plot = ggplot(res) + theme_bw()+ theme_bw() + 
    #geom_point(aes(Age , actual))+
    #geom_point(aes(Age , expected)) + 
    geom_line(aes(Age , actual, linetype ="Actual" ))+
    geom_line(aes(Age , expected, linetype ="Expected" )) + 
    facet_wrap(~Gender) + 
    labs(title = "GLM Model: Actual vs. Expected Incidence Rates\nValidation Set",
         linetype="Legend")
  
  res = incidence$val %>%
    group_by(Duration) %>%
    summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
              preds =sum(preds)) %>%
    mutate(Actual = Count_NH / AdjustedExposure, Predicted = preds / AdjustedExposure, 
           AE = Actual/Predicted)
  total_ae = sum(res$Count_NH) / sum(res$preds)
  res$mean_ae = total_ae
  duration_plot = ggplot(res) + 
    theme_bw() + geom_line(aes(Duration , Actual, linetype="Actual"))+ 
    geom_line(aes(Duration , Predicted, linetype="Predicted")) +
    labs(title = "Actual vs. Predicted Incidence Rates by Duration\nValidation Set", 
         y= "Incidence Rate", x="Duration", linetype="Legend") +
    coord_cartesian(xlim = c(1,20), ylim=c(0,.05))
  ae_plot = ggplot(res) + theme_bw() + geom_line(aes(Duration , AE, linetype="Actual/Predicted"))+
    geom_line(aes(Duration,mean_ae, linetype="Mean Ratio")) + 
    labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration\nValidation Set", 
         x= "Duration", y= "Actual / Predicted Ratio", linetype="Legend") + 
    coord_cartesian(xlim = c(1,20), ylim = c(0.8,1.2))
  
  
  # Calculate NPLL
  pnll = poisson_neg_log_lik(y_pred = preds, y_true = incidence$val$Count_NH)
  res_rmse = rmse(preds-incidence$val$Count_NH)
  
  preds = predict(object = mod_glm, newdata = incidence$test, type = 'response')
  pnll_test = poisson_neg_log_lik(y_pred = preds, y_true = incidence$test$Count_NH)
  res_rmse_test = rmse(preds-incidence$test$Count_NH)
  
  list(pnll = pnll, rmse=res_rmse, pnll_test= pnll_test, res_rmse_test = res_rmse_test, 
       total_ae = total_ae, aic = mod_glm$aic,
       rate_plot = rate_plot, duration_plot=duration_plot, ae_plot=ae_plot, 
       time_taken = time_taken)
   
  
}

```



### Run options for GLM


```{r}
# Get Data
set.seed(123)


ans_full = run_glm(formula, speed = FALSE) # Use all major predictors 
ans_no_interactions = run_glm(formula_no_interactions, speed = FALSE) # Use all major predictors with no interactions
ans_soa = run_glm(formula_soa,speed = FALSE) # Use the SOA predictors
ans=list(ans_soa=ans_soa, ans_no_interactions=ans_no_interactions, ans_full=ans_full)
ans
save(ans, file = p0(output_dir, "glm_results.RData"))
#load(file = p0(output_dir, "glm_results.RData"))
unlist(map(ans, function(x) x$pnll))
results = data.frame(PNLL_Validation = unlist(map(ans, `[`, 'pnll')), RMSE_Validation = unlist(map(ans, `[`, 'rmse')),
                     PNLL_Test = unlist(map(ans, `[`, 'pnll_test')), RMSE_Test = unlist(map(ans, `[`, 'res_rmse_test')))
                     
print(results)
rm(list=c("ans", "ans_full", "ans_no_interactions", "ans_soa"))
```


# Lasso

Note: The cross validation routine can take hours to run even on multiple processors

### Lasso on All Train Data

```{r}
# Get Data
set.seed(123)


#Formula 1
# excludes StateAbbr, RateIncreaseFlag, IssueAgeBucket
formula = Count_NH ~ offset(log(ActiveExposure)) + 
  (Gender + IncurredAgeBucket  + PolicyYear + Marital_Status + Prem_Class + 
     Underwriting_Type + Cov_Type_Bucket  + NH_Orig_Daily_Ben_Bucket + NH_Ben_Period_Bucket + 
     NH_EP_Bucket )^2 + TQ_Status + Region + IssueYear  + Infl_Rider + I(Duration^2)

# Create a matrix of predictors on the training set, excluding the intercept
design_train = model.matrix(formula, incidence$train)[,-1]  


# Run Lasso with Cross Validation
a=Sys.time()
cv_mod_glm= cv.glmnet(x = design_train, y = incidence$train$Count_NH,
                      offset = log(incidence$train$ActiveExposure),
                      nfolds = 5, parallel = TRUE,
                      family = "poisson")
b=Sys.time(); b-a
save(cv_mod_glm, file=p0(output_dir,"cv_mod_glm_lasso.RData")) # formula 1
#load(file=p0(output_dir,"cv_mod_glm_lasso.RData")) # formula 1
plot(cv_mod_glm)
(cv_mod_glm$lambda.min) # lambda with the lowest cross-validation error
(cv_mod_glm$lambda.1se) # largest value of lambda such that error is within 1 standard error of the minimum.
coef_summary = coef(cv_mod_glm, s = "lambda.1se")

# Create a matrix of predictors on the validation set, excluding the intercept
design_val = model.matrix(formula, incidence$val)[,-1]

# Create a matrix of predictors on the validation set, excluding the intercept
design_test = model.matrix(formula, incidence$test)[,-1]



#Calculate the predictions at the lambda.1se level
preds = predict(object = cv_mod_glm, s = "lambda.1se", newx = design_val, 
                newoffset = log(incidence$val$ActiveExposure), type = 'response')
incidence$val$preds=c(preds)

# Produce Plots
res = incidence$val %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, 
         expected = preds / AdjustedExposure)

rate_plot = ggplot(res) + theme_bw() + geom_line(aes(Age , actual, linetype="Actual"))+
  geom_line(aes(Age , expected, linetype="Predicted")) + 
  facet_wrap(~Gender) + labs(title = "Lasso Model: Actual vs. Expected Incidence Rates by Gender", linetype= "Legend")

res = incidence$val %>%
  group_by(PolicyYear) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
            preds =sum(preds)) %>%
  mutate(Actual = Count_NH / AdjustedExposure, Predicted = preds / AdjustedExposure, 
         AE = Actual/Predicted)
total_ae = sum(res$Count_NH) / sum(res$preds)
res$mean_ae = total_ae
duration_plot = ggplot(res) + theme_bw()+
  geom_line(aes(PolicyYear , Actual, linetype="Actual"))+ 
  geom_line(aes(PolicyYear , Predicted, linetype="Predicted")) +
  labs(title = "Actual vs. Predicted Incidence Rates by Duration", y= "Incidence Rate", x="Duration", linetype="Legend") +
  coord_cartesian(xlim = c(0,20), ylim=c(0,.05))
ae_plot = ggplot(res) + theme_bw()+ geom_line(aes(PolicyYear , AE, linetype="Actual/Predicted"))+
  geom_line(aes(PolicyYear,mean_ae, linetype="Mean Ratio")) + 
  labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration", x= "Duration", 
       y= "Actual/Predicted Ratio", linetype="Legend") + 
  coord_cartesian(xlim = c(0,25), ylim = c(0.6,1.2))

  
# Calculate NPLL
pnll = poisson_neg_log_lik(y_pred = preds, y_true = incidence$val$Count_NH)
res_rmse = rmse(preds-incidence$val$Count_NH)

preds = predict(object = cv_mod_glm, s = "lambda.1se", newx = design_test, 
              newoffset = log(incidence$test$ActiveExposure), type = 'response')%>% c()

pnll_test = poisson_neg_log_lik(y_pred = preds, y_true = incidence$test$Count_NH)
res_rmse_test = rmse(preds-incidence$test$Count_NH)

ans = list(coef_summary=coef_summary, rate_plot=rate_plot, duration_plot=duration_plot, ae_plot=ae_plot, pnll=pnll, res_rmse=res_rmse, pnll_test=pnll_test, res_rmse_test=res_rmse_test)

print(ans)
```


### Lasso on 10% of Train Data

```{r}

# Get Data
incidence = get_incidence()
selected_N = floor(0.1 *nrow(incidence$train)) # training data is already shuffled, so there is no need to reshuffle
incidence$train = incidence$train[1:selected_N,]
set.seed(123)


#Formula 1
# excludes StateAbbr, RateIncreaseFlag, IssueAgeBucket
formula = Count_NH ~ offset(log(ActiveExposure)) + 
  (Gender + IncurredAgeBucket  + PolicyYear + Marital_Status + Prem_Class + 
     Underwriting_Type + Cov_Type_Bucket  + NH_Orig_Daily_Ben_Bucket + NH_Ben_Period_Bucket + 
     NH_EP_Bucket )^2 + TQ_Status + Region + IssueYear  + Infl_Rider + I(Duration^2)

# Create a matrix of predictors on the training set, excluding the intercept
design_train = model.matrix(formula, incidence$train)[,-1]  


# Run Lasso with Cross Validation
a=Sys.time()
cv_mod_glm= cv.glmnet(x = design_train, y = incidence$train$Count_NH,
                      offset = log(incidence$train$ActiveExposure),
                      nfolds = 5, parallel = TRUE,
                      family = "poisson")
b=Sys.time(); b-a
save(cv_mod_glm, file=p0(output_dir,"cv_mod_glm_lasso_limited_data.RData")) # formula 1
#load(file=p0(output_dir,"cv_mod_glm_lasso_limited_data.RData")) # formula 1
plot(cv_mod_glm)
(cv_mod_glm$lambda.min) # lambda with the lowest cross-validation error
(cv_mod_glm$lambda.1se) # largest value of lambda such that error is within 1 standard error of the minimum.
coef_summary = coef(cv_mod_glm, s = "lambda.1se")

# Create a matrix of predictors on the validation set, excluding the intercept
design_val = model.matrix(formula, incidence$val)[,-1]

# Create a matrix of predictors on the validation set, excluding the intercept
design_test = model.matrix(formula, incidence$test)[,-1]



#Calculate the predictions at the lambda.1se level
preds = predict(object = cv_mod_glm, s = "lambda.1se", newx = design_val, 
                newoffset = log(incidence$val$ActiveExposure), type = 'response')
incidence$val$preds=c(preds)

# Produce Plots
res = incidence$val %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, 
         expected = preds / AdjustedExposure)

rate_plot = ggplot(res) + theme_bw() + geom_line(aes(Age , actual, linetype="Actual"))+
  geom_line(aes(Age , expected, linetype="Predicted")) + 
  facet_wrap(~Gender) + labs(title = "Lasso Model: Actual vs. Expected Incidence Rates by Gender", linetype= "Legend")

res = incidence$val %>%
  group_by(PolicyYear) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
            preds =sum(preds)) %>%
  mutate(Actual = Count_NH / AdjustedExposure, Predicted = preds / AdjustedExposure, 
         AE = Actual/Predicted)
total_ae = sum(res$Count_NH) / sum(res$preds)
res$mean_ae = total_ae
duration_plot = ggplot(res) + theme_bw()+ geom_line(aes(PolicyYear , Actual, linetype="Actual"))+ 
  geom_line(aes(PolicyYear , Predicted, linetype="Predicted")) +
  labs(title = "Actual vs. Predicted Incidence Rates by Duration", y= "Incidence Rate", x="Duration", linetype= "Legend") +
  coord_cartesian(xlim = c(0,20), ylim=c(0,.05))
ae_plot = ggplot(res) + theme_bw()+ geom_line(aes(PolicyYear , AE, linetype="Actual/Predicted"))+
  geom_line(aes(PolicyYear,mean_ae, linetype="Mean AE")) + 
  labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration", x= "Duration") + 
  coord_cartesian(xlim = c(0,25), ylim = c(0.8,1.2))

  
# Calculate NPLL
pnll = poisson_neg_log_lik(y_pred = preds, y_true = incidence$val$Count_NH)
res_rmse = rmse(preds-incidence$val$Count_NH)

preds = predict(object = cv_mod_glm, s = "lambda.1se", newx = design_test, 
              newoffset = log(incidence$test$ActiveExposure), type = 'response')%>% c()

pnll_test = poisson_neg_log_lik(y_pred = preds, y_true = incidence$test$Count_NH)
res_rmse_test = rmse(preds-incidence$test$Count_NH)

ans = list(coef_summary=coef_summary, rate_plot=rate_plot, duration_plot=duration_plot, ae_plot=ae_plot, pnll=pnll, res_rmse=res_rmse, pnll_test=pnll_test, res_rmse_test=res_rmse_test)

print(ans)
```


# Neural Networks

### Define Custom Layers

```{r}
# Create a custome layer for taking the exponent of each value


K <- backend()
k_poisson_neg_log_lik = function(y_true, y_pred){
  
  eps=1e-8
  
  return(K$mean(y_pred - y_true * K$log(y_pred+eps)))
}
ExpLayer <- R6::R6Class("CustomLayer",
                        inherit = KerasLayer,
                        public = list(
                          output_dim = NULL,
                          kernel = NULL,
                          initialize = 
                            function(output_dim) {
                              self$output_dim <- output_dim
                            },
                          call = function(x, mask = NULL) {
                            k_exp(x)
                          }
                        ))
layer_exp <- function(object, name = "exponent", trainable = FALSE) {
  create_layer(ExpLayer, object, list(
    output_dim = as.integer(1),
    name = name,
    trainable = trainable
  ))
}


```


### Prepare Data for Neural Network

```{r}
# Load Database
incidence = get_incidence()
#incidence = get_incidence(train_val_test = c(0.1,0.1,0.1))

# Scale all values except exposure and counts
incidence_scaled = scale_data(incidence)
incidence = incidence_scaled$incidence; 
scale_mean = incidence_scaled$scale_mean; 
scale_sd = incidence_scaled$scale_sd
rm(incidence_scaled)

```

### Single Node Single Layer Neural Network (Equivalent to GLM)


```{r}

# Main GLM Formula
# excludes StateAbbr, RateIncreaseFlag, IssueAgeBucket
formula = Count_NH ~ -1 + offset(log(ActiveExposure)) +
  (Gender + IssueYear + IncurredAgeBucket  + PolicyYear + Marital_Status + Prem_Class + 
      Underwriting_Type + Cov_Type_Bucket + TQ_Status + NH_Orig_Daily_Ben_Bucket + NH_Ben_Period_Bucket + 
      NH_EP_Bucket  + Region)^2   + Infl_Rider + I(Duration^2) 


## Note: Keras adds an intercept term in automatically, so it is removed from the above formula in order not to be duplicated

# Create design matrixes and offsets for input into model
design = model.matrix(formula, incidence$train)
offset =log (incidence$train$ActiveExposure)
y = incidence$train$Count_NH
design_val = model.matrix(formula, incidence$val)
offset_val =log (incidence$val$ActiveExposure) 
y_val = incidence$val$Count_NH
design_test = model.matrix(formula, incidence$test)
offset_test =log (incidence$test$ActiveExposure) 
y_test = incidence$test$Count_NH

N_design= nrow(design)
N_predictors = ncol(design)


# Define Network

# The input layers are specified (blue circles in chart) 
input_offset <- layer_input(shape = c(1), name = "input_offset")
input_predictors <- layer_input(shape = c(N_predictors), 
                                name = "input_predictors")

# Single node, single layer (green circle in chart)
rate_mean = input_predictors %>%
  layer_dense(units =1, activation = 'linear') 
  #k_exp()

# Prediction is made (purple and orange circles in chart)
predictions = layer_add(c(input_offset, rate_mean)) %>%
  layer_exp()

# The model is defined linking the inputs to the outputs
model <- keras_model(inputs = c(input_offset, input_predictors), 
                     outputs = predictions)

# Summary of the model
summary(model)

# Model is compiled
model %>% compile(
  loss = 'poisson',
  optimizer = optimizer_rmsprop(),
  metrics = c('mse') # 
)

# Run Model
a=Sys.time()
history <- model %>% fit(
  x = list(input_offset = offset, input_predictors = design),
  y= y,
  epochs = 15, batch_size = 2048*8, 
  validation_split = 0.2,
  verbose = 1
)
b=Sys.time(); b-a
history_data = data.frame(epoch = 1:history$params$epochs, 
                          training_loss = history$metrics$loss, validation_loss = history$metrics$val_loss)

ggplot(history_data[1:5,]) + theme_bw()+
  geom_line(aes(epoch, training_loss, linetype = "Training")) +
  geom_line(aes(epoch, validation_loss, linetype = "Validation")) +
  labs(title = "Training and Validation Loss by Epoch", x= "Epoch", y="NPLL", linetype="Data Source")

## Val Log Likelihood:  Score
preds = model %>% predict(
  x=list(input_offset = offset_val, 
         input_predictors=design_val),
  verbose=1, batch_size = 2048*8
) %>% c()
npll_val = poisson_neg_log_lik(y_pred = preds, y_true = y_val, eps=1e-8)
rmse_val = rmse(preds-y_val)


## Validation Data
incidence$val$preds=preds

res = incidence$val %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, expected = preds / AdjustedExposure)
res
plot_age_val = ggplot(res) + theme_bw() + geom_line(aes(Age , actual, linetype="actual"))+ 
  geom_line(aes(Age , expected, linetype="expected")) + 
  facet_wrap(~Gender)
res = incidence$val %>%
  group_by(Duration) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
            preds = sum(preds)) %>%
  mutate(Actual = Count_NH / AdjustedExposure, Predicted = preds / AdjustedExposure, AE = Actual/Predicted)
res$mean_ae = sum(res$Count_NH) / sum(res$preds)
plot_duration_val = ggplot(res) + theme_bw()+ 
  geom_line(aes(Duration , Actual, linetype="Actual")) + 
  geom_line(aes(Duration , Predicted, linetype="Predicted")) +
  labs(title = "Actual vs. Predicted Incidence Rates by Duration", y= "Incidence Rate", linetype = "Legend") +
  coord_cartesian(xlim = c(0,20), ylim=c(0,.05))
plot_ae_val = ggplot(res) + theme_bw() + geom_line(aes(Duration , AE, linetype = "Actual/Predicted"))+
  geom_line(aes(Duration,mean_ae, linetype="Mean Ratio")) + 
  labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration", 
       linetype = "Legend",
       y="Actual / Predicted Ratio",x= "Duration") + 
  coord_cartesian(xlim = c(0,25), ylim = c(0.8,1.2))

ae_val = sum(res$Count_NH) / sum(res$preds)

## Test Data
preds = model %>% predict(
  x=list(input_offset = offset_test, 
         input_predictors=design_test),
  verbose=1, batch_size = 2048*8
) %>% c()
npll_test = poisson_neg_log_lik(y_pred = preds, y_true = y_test, eps=1e-8)
incidence$test$preds=preds
res = incidence$test %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, expected = preds / AdjustedExposure)
res
plot_age_test = ggplot(res) +  theme_bw()+geom_line(aes(Age , actual, linetype="actual"))+ 
  geom_line(aes(Age , expected, linetype="expected")) + 
  facet_wrap(~Gender)
res = incidence$test %>%
  group_by(Duration) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
            preds = sum(preds)) %>%
  mutate(Actual = Count_NH / AdjustedExposure, 
         Predicted = preds / AdjustedExposure, AE = Actual/Predicted)
res$mean_ae = sum(res$Count_NH) / sum(res$preds)
plot_duration_test = ggplot(res) +  theme_bw()+geom_line(aes(Duration , Actual, linetype="Actual"))+ 
  geom_line(aes(Duration , Predicted, linetype="Predicted")) +
  labs(title = "Actual vs. Predicted Incidence Rates by Duration", y= "Incidence Rate", x= "Duration") + coord_cartesian(xlim = c(0,25), ylim=c(0,.05))
plot_ae_test = ggplot(res) + theme_bw()+ geom_line(aes(Duration , AE, linetype="Actual/Predicted"))+
  geom_line(aes(Duration,mean_ae, linetype="Mean Ratio")) + 
  labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration", 
       y= "Actual / Predicted Ratio", linetype = "Legend") + coord_cartesian(xlim = c(0,25), ylim = c(0.8,1.2))
ae_test = sum(res$Count_NH) / sum(res$preds)

ans = list(npll_val = npll_val, plot_age_val=plot_age_val, plot_duration_val=plot_duration_val, 
           plot_ae_val=plot_ae_val, ae_val= ae_val, 
           npll_test = npll_test, plot_age_test=plot_age_test, plot_duration_test= plot_duration_test, 
           plot_ae_test=plot_ae_test, ae_test=ae_test)

print(ans)
    
```


###  Multi-Layer Neural Network


```{r}
formula = Count_NH ~ . -1 - StateAbbr - RateIncreaseFlag

design = model.matrix(formula, incidence$train[,c(27,1:23)])
offset =log (incidence$train$ActiveExposure)
y = incidence$train$Count_NH
N_predictors = ncol(design)

design_val = model.matrix(formula, incidence$val[,c(27,1:23)])
offset_val =log (incidence$val$ActiveExposure) 
y_val = incidence$val$Count_NH

design_test = model.matrix(formula, incidence$test[,c(27,1:23)])
offset_test =log (incidence$test$ActiveExposure) 
y_test = incidence$test$Count_NH


input_offset <- layer_input(shape = c(1), name = "input_offset")
input_predictors <- layer_input(shape = c(N_predictors), name = "input_predictors")

rate_mean = input_predictors %>%
  layer_dense(units =30, activation = 'relu') %>%
  layer_dropout(0.02) %>% #.01
  layer_dense(units =15, activation = 'relu') %>% #10
  layer_dropout(0.02) %>% #.01
  layer_dense(units =1, activation = 'linear')
  

predictions = layer_add(c(input_offset, rate_mean)) %>%
  layer_exp()

# We define a trainable model linking the tweet inputs to the predictions
model <- keras_model(inputs = c(input_offset, input_predictors), outputs = predictions)

summary(model)

model %>% compile(
  loss = 'poisson',
  optimizer = optimizer_rmsprop(),
  metrics = c('mse') # k_poisson_neg_log_lik
)

#k_set_value(model$optimizer$lr, 1e-5)

history <- model %>% fit(
  x = list(input_offset = offset, input_predictors = design),
  y= y,
  epochs = 3, batch_size = 2048*10, 
  validation_split = 0.2,
  verbose = 1
)
k_set_value(model$optimizer$lr, 1.5e-2)
history <- model %>% fit(
  x = list(input_offset = offset, input_predictors = design),
  y= y,
  epochs = 5, batch_size = 2048*10, 
  validation_split = 0.2,
  verbose = 1
)
k_set_value(model$optimizer$lr, 1e-3)
history <- model %>% fit(
  x = list(input_offset = offset, input_predictors = design),
  y= y,
  epochs = 5, batch_size = 2048*10, 
  validation_split = 0.2,
  verbose = 1
)
k_set_value(model$optimizer$lr, 1e-4)
history <- model %>% fit(
  x = list(input_offset = offset, input_predictors = design),
  y= y,
  epochs = 5, batch_size = 2048*10, 
  validation_split = 0.2,
  verbose = 1
)

## Val Log Likelihood:  Score
preds = model %>% predict(
  x=list(input_offset = offset_val, 
         input_predictors=design_val),
  verbose=1, batch_size = 2048*8
) %>% c()
npll_val = poisson_neg_log_lik(y_pred = preds, y_true = y_val, eps=1e-8)
rmse_val = rmse(preds-y_val)


## Validation Data
incidence$val$preds=preds

res = incidence$val %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, expected = preds / AdjustedExposure)
res
plot_age_val = ggplot(res) +  theme_bw()+geom_line(aes(Age , actual, linetype="actual"))+ 
  geom_line(aes(Age , expected, linetype="expected")) + 
  facet_wrap(~Gender)
res = incidence$val %>%
  group_by(Duration) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
            preds = sum(preds)) %>%
  mutate(Actual = Count_NH / AdjustedExposure, Predicted = preds / AdjustedExposure, AE = Actual/Predicted)
res$mean_ae = sum(res$Count_NH) / sum(res$preds)
plot_duration_val = ggplot(res) +  theme_bw()+geom_line(aes(Duration , Actual, linetype="Actual")) + 
  geom_line(aes(Duration , Predicted, linetype="Predicted")) +
  labs(title = "Actual vs. Predicted Incidence Rates by Duration", y= "Incidence Rate") +
  coord_cartesian(xlim = c(0,20), ylim=c(0,.05))
plot_ae_val = ggplot(res) +  theme_bw()+geom_line(aes(Duration , AE, linetype="Actual/Predicted"))+
  geom_line(aes(Duration,mean_ae, linetype="Mean AE")) + 
  labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration") + 
  coord_cartesian(xlim = c(0,25), ylim = c(0.8,1.2))

ae_val = sum(res$Count_NH) / sum(res$preds)

## Test Data
preds = model %>% predict(
  x=list(input_offset = offset_test, 
         input_predictors=design_test),
  verbose=1, batch_size = 2048*8
) %>% c()
npll_test = poisson_neg_log_lik(y_pred = preds, y_true = y_test, eps=1e-8)
incidence$test$preds=preds
res = incidence$test %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, expected = preds / AdjustedExposure)
res
plot_age_test = ggplot(res) + theme_bw()+ geom_line(aes(Age , actual, linetype="actual"))+ 
  geom_line(aes(Age , expected, linetype="expected")) + 
  facet_wrap(~Gender)
res = incidence$test %>%
  group_by(Duration) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
            preds = sum(preds)) %>%
  mutate(Actual = Count_NH / AdjustedExposure, 
         Predicted = preds / AdjustedExposure, AE = Actual/Predicted)
res$mean_ae = sum(res$Count_NH) / sum(res$preds)
plot_duration_test = ggplot(res) + theme_bw()+ geom_line(aes(Duration , Actual, linetype="Actual"))+ 
  geom_line(aes(Duration , Predicted, linetype="Predicted")) +
  labs(title = "Actual vs. Predicted Incidence Rates by Duration", y= "Incidence Rate") + coord_cartesian(xlim = c(0,25), ylim=c(0,.05))
plot_ae_test = ggplot(res) + theme_bw()+ geom_line(aes(Duration , AE, linetype="Actual/Predicted"))+
  geom_line(aes(Duration,mean_ae, linetype="Mean AE")) + 
  labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration", y= "ACtual / Expected Ratio") + coord_cartesian(xlim = c(0,25), ylim = c(0.8,1.2))
ae_test = sum(res$Count_NH) / sum(res$preds)

ans = list(npll_val = npll_val, plot_age_val=plot_age_val, plot_duration_val=plot_duration_val, 
           plot_ae_val=plot_ae_val, ae_val= ae_val, 
           npll_test = npll_test, plot_age_test=plot_age_test, plot_duration_test= plot_duration_test, 
           plot_ae_test=plot_ae_test, ae_test=ae_test)
ans

```

# XGBoost Model

### Prepare the Data

```{r}
 incidence = get_incidence()
  set.seed(123)

  # Set up data
  offset_train =log (incidence$train$ActiveExposure) 
  y_train = incidence$train$Count_NH
  
  offset_val =log (incidence$val$ActiveExposure) 
  y_val = incidence$val$Count_NH
  
  offset_test =log (incidence$test$ActiveExposure) 
  y_test = incidence$test$Count_NH
  
  dtrain <- xgb.DMatrix(data=as.matrix(map_df(incidence$train[,c(1:11, 13:22)], as.numeric)), 
                        label=y_train)
  setinfo(dtrain,"base_margin",offset_train) # Identifies offset
  
  dval <- xgb.DMatrix(data=as.matrix(map_df(incidence$val[,c(1:11, 13:22)], as.numeric)), 
                       label = y_val)
  setinfo(dval,"base_margin",offset_val)# Identifies offset

  dtest <- xgb.DMatrix(data=as.matrix(map_df(incidence$test[,c(1:11, 13:22)], as.numeric)), 
                       label = y_test)
  setinfo(dtest,"base_margin",offset_test)# Identifies offset

```


### Set the Initial Parameters

```{r}
# Paramaters for xgboost
params <- list(objective="count:poisson",
               eval_metric = "poisson-nloglik",
               booster = "gbtree",
               eta = .03,
               gamma = 1,
               max_depth = 4,
               min_child_weight = 1,
               subsample = .8,
               colsample_bytree = .8,
               nthread = 10 # set to the number of threads available on your system
)

```

### Run the Cross Validation (if required)

```{r}
xgb_cv <- xgb.cv(data = dtrain,
                   params = params,
                   nrounds = 2000,
                   maximize = FALSE,
                   prediction = TRUE,
                   print_every_n = 25,
                   early_stopping_rounds = 50,
                   nfold= 5
  )

  
  # Increase the nrounds because xgboost will run on both training and cv data used in xgb.cv 
  (nrounds = xgb_cv$best_iteration * 1.1)
save(xgb_cv , file=paste0(output_dir, "xgb_cv.RData"))    
```


###  Run XGBoost

```{r}

# Establish the nrounds parameter
nrounds = 2200 # Choose the appropriate value from the cross validation process
 set.seed(123)
 
## Run the Model
mod <- xgboost(data = dtrain,params = params, nrounds = nrounds)

xgb.save(mod, p0(output_dir, "xgb_mod.RData"))
#mod = xgb.load(modelfile =  p0(output_dir, "xgb_mod.RData"))

#Check the in-sample error
preds_train <- predict(mod, dtrain)
poisson_neg_log_lik(y_pred = preds_train, y_true = y_train, eps=0)
rmse(preds_train - y_train)


########################
# Perform validation set predictions
preds_val <- predict(mod, dval) 
incidence$val$preds=preds_val
(pnll_val = poisson_neg_log_lik(y_pred = preds_val, y_true = y_val, eps=0))

res = incidence$val %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, expected = preds / AdjustedExposure)
res
plot_age_val = ggplot(res) + theme_bw()+ geom_line(aes(Age , actual, linetype="actual"))+ 
  geom_line(aes(Age , expected, linetype="expected")) + 
  facet_wrap(~Gender)
res = incidence$val %>%
  group_by(Duration) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
            preds = sum(preds)) %>%
  mutate(Actual = Count_NH / AdjustedExposure, Predicted = preds / AdjustedExposure, AE = Actual/Predicted)

res$mean_ae = sum(res$Count_NH) / sum(res$preds)
plot_duration_val = ggplot(res) + theme_bw()+ geom_line(aes(Duration , Actual, linetype="Actual")) + 
  geom_line(aes(Duration , Predicted, linetype="Predicted")) +
  labs(title = "Actual vs. Predicted Incidence Rates by Duration", y= "Incidence Rate") +
  coord_cartesian(xlim = c(0,20), ylim=c(0,.05))
plot_ae_val = ggplot(res) + theme_bw()+ geom_line(aes(Duration , AE, linetype="Actual/Predicted"))+
  geom_line(aes(Duration,mean_ae, linetype="Mean AE")) + 
  labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration") + 
  coord_cartesian(xlim = c(0,25), ylim = c(0.8,1.2))

res = incidence$val %>%
  group_by(Age, IssueYear) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, expected = preds / AdjustedExposure)
plot_issue_year = ggplot(res) + theme_bw()+ geom_line(aes(Age , actual, linetype="Actual"))+ 
  geom_line(aes(Age , expected, linetype="Predicted")) + 
  facet_wrap(~ IssueYear, scales = "free") + theme_bw()+
  labs(title = "Actual vs. Predicted Rates by Issue Year Band\nValidation Set", 
       y="Incidence Rate", x="Incurred Age", linetype= "Legend")
print(plot_issue_year)
ggplot(res) + theme_bw()+ geom_line(aes(Age , expected, linetype=factor(IssueYear))) + labs(title="Expected") + facet_wrap(~ IssueYear, scales = "free")
ggplot(res) + theme_bw()+ geom_line(aes(Age , actual, linetype=factor(IssueYear)))+ labs(title="Actual")


ae_val = sum(res$Count_NH) / sum(res$preds)



## Test Data
########################
# Perform test set predictions
preds_test <- predict(mod, dtest) 
incidence$test$preds=preds_test
(pnll_test = poisson_neg_log_lik(y_pred = preds_test, y_true = y_test, eps=0))

res = incidence$test %>%
  group_by(Gender,Age) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, expected = preds / AdjustedExposure)
res
plot_age_test = ggplot(res) + theme_bw()+ geom_line(aes(Age , actual, linetype="actual"))+ 
  geom_line(aes(Age , expected, linetype="expected")) + 
  facet_wrap(~Gender)
res = incidence$test %>%
  group_by(Duration) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH), 
            preds = sum(preds)) %>%
  mutate(Actual = Count_NH / AdjustedExposure, Predicted = preds / AdjustedExposure, AE = Actual/Predicted)

res$mean_ae = sum(res$Count_NH) / sum(res$preds)
plot_duration_test = ggplot(res) + theme_bw()+ geom_line(aes(Duration , Actual, linetype="Actual")) + 
  geom_line(aes(Duration , Predicted, linetype="Predicted")) +
  labs(title = "Actual vs. Predicted Incidence Rates by Duration", y= "Incidence Rate") +
  coord_cartesian(xlim = c(0,20), ylim=c(0,.05))
plot_ae_test = ggplot(res) + theme_bw()+ geom_line(aes(Duration , AE, linetype="Actual/Predicted"))+
  geom_line(aes(Duration,mean_ae, linetype="Mean AE")) + 
  labs(title = "Ratio of Actual / Predicted Incidence Rates by Duration") + 
  coord_cartesian(xlim = c(0,25), ylim = c(0.8,1.2))

res = incidence$test %>%
  group_by(Age, IssueYear) %>%
  summarize(AdjustedExposure = sum(AdjustedExposure), Count_NH = sum(Count_NH),preds = sum(preds)) %>%
  mutate(ae = Count_NH / preds, actual = Count_NH / AdjustedExposure, expected = preds / AdjustedExposure)
plot_issue_year_test = ggplot(res) + theme_bw()+ geom_line(aes(Age , actual, linetype="Actual"))+ 
  geom_line(aes(Age , expected, linetype="Predicted")) + 
  facet_wrap(~ IssueYear, scales = "free") + theme_bw()+
  labs(title = "Actual vs. Predicted Rates by Issue Year Band\nTest Set", 
       y="Incidence Rate", x="Incurred Age", linetype= "Legend")
print(plot_issue_year_test)
ggplot(res) + theme_bw()+ geom_line(aes(Age , expected, linetype=factor(IssueYear))) + labs(title="Expected") + facet_wrap(~ IssueYear, scales = "free")
ggplot(res) + theme_bw()+ geom_line(aes(Age , actual, linetype=factor(IssueYear)))+ labs(title="Actual")



ae_test = sum(res$Count_NH) / sum(res$preds)



```

